{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Day2-ex-adversarial-examples.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOG/GAATXGWu1Pwn0D47Wqx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eHwHlo01n2E6","executionInfo":{"status":"ok","timestamp":1629200211753,"user_tz":-540,"elapsed":21565,"user":{"displayName":"Hiroshi Kera","photoUrl":"","userId":"01053422096626415082"}},"outputId":"b22489ef-a42b-4540-fe88-ff927711072e"},"source":["# Google Driveのマウント\n","from google.colab import drive\n","drive.mount('/content/drive/')\n","\n","# 目的の場所（フォルダ・ディレクトリ）へ移動（各自の環境で適宜修正）\n","%cd \"/content/drive/MyDrive/Colab Notebooks/情報工学実験1A/\"\n","%ls"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive/\n","/content/drive/MyDrive/Colab Notebooks/情報工学実験1A\n","\u001b[0m\u001b[01;34mbackyard\u001b[0m/  \u001b[01;34mdata\u001b[0m/  \u001b[01;34mmodel\u001b[0m/  \u001b[01;34mnotebook\u001b[0m/  \u001b[01;34msrc_day1\u001b[0m/  \u001b[01;34msrc_day1_ans\u001b[0m/\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"MXMQL_r3DQxf"},"source":["\n","---\n","# 課題"]},{"cell_type":"markdown","metadata":{"id":"5TLN9lpXDpQ6"},"source":["## 課題１（重要）\n","**この課題の結果は次回の実験で利用するので必ず行うこと**\n","\n","実験課題1, 2をFGSMでなくPGDで行え．\n","- PGDでATしたモデルは，FGSMの時と同様，`model_cifar10_pgd0.03.pth`の様にして保存せよ．今後の便利のため，`alpha`, `n_iter`の値はファイル名に含めない．"]},{"cell_type":"code","metadata":{"id":"fFAF9i3vPUrH"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1KjKk5hyKsyQ"},"source":["## 課題２\n","\n","これまで説明した敵対的サンプルの生成方法（例えばFGSM）では，「正しいラベルでの誤差が大きくなるように」画像を変化させ敵対的サンプルを生成した．これはUntargetedな方法（例えばUntargeted FGSM）と呼ばれる．この場合，敵対的サンプルがどの誤ったラベルになるかは事前にわからない．これに対してTargetedな方法は，「指定した誤ったラベルでの誤差が小さくなるように」画像を変化させる．Untargetedな方法が実装できていれば，ほんの少し修正すればTargetedな方法に修正できる．Targeted FGSM, Targeted PGDを実装し，`adversarial_attack.py`に追加せよ．Targetedな方法でいくつか敵対的サンプルを生成し，うまくいっていることを確認せよ（練習問題１の様な感じで）．"]},{"cell_type":"code","metadata":{"id":"zarOxm55PV-G"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u6h2PwRBMZ6L"},"source":["## 課題３（最終レポート課題）\n","**この内容は最終レポートに含める課題の一つとする**\n","\n","Adversarial attackは「人間の目には見えないほど小さな変化で分類器を騙す」というものである．この「人間の目には見えないほど」という制限を無くしたらどのようになるだろうか？\n","\n","攻撃強度`epsilon`の制限を極端にゆるくした敵対的サンプルを生成し画像として表示し，その様子を観察せよ．`image`と`image + delta`の画像の間でどの様な変化が起きているだろうか？それぞれの画像に対しネットワークが出力した分類結果（クラス名）も並べて観察せよ（Day2 練習問題1の`cmp_images()`が参考になる）．通常の学習をしたモデル，ATをしたモデルそれぞれについて同様に制限の緩い敵対的サンプルを生成し，その違いについても議論せよ．課題2のTargetedな生成方法だともう少し顕著な結果が出る．\n","\n","ここでの敵対的サンプル生成は，PGDで`epsilon=1.0`, `alpha=0.01`, `n_iter=100`あたりが良い（学習したモデルによるので，適宜修正せよ）．\n","\n","---\n","追加課題（興味があればやってみよ）：\n","- 今回のAdversarial Attackは全て$L_{\\infty}$ノルムに基づくものである．今回の課題は，$L_2$ノルムに基づくものだとより綺麗に結果が出る（ATも攻撃サンプル生成も$L_2$ノルムベースでやる）．`L1ノルム，L2ノルム`などで検索し$L_{p}$ノルムが何か把握した上で，調査しATの$L_2$ノルム版を実装し，上の課題をやってみよ．"]},{"cell_type":"code","metadata":{"id":"dcWFr0qL-Szt"},"source":[""],"execution_count":null,"outputs":[]}]}